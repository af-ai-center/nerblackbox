{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSTREAM_TASK = 'ner'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "APPLICATION OF SWEDISH BERT MODEL FOR TASK: BertForTokenClassification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: \n",
    "Before running this notebook, make sure to have followed the steps explained in the \"Setup\" section of README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "\n",
    "from utils.utils import get_available_models\n",
    "from utils.utils import get_available_datasets\n",
    "from utils.utils import prune_examples\n",
    "from utils.utils import ENV_VARIABLE\n",
    "from utils.utils import preprocess_data\n",
    "from utils.utils import get_dataset_path\n",
    "from utils.utils import save_model_checkpoint\n",
    "from utils.utils import save_metrics\n",
    "from utils.ner_trainer import NERTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PRETRAINED_MODELS = ENV_VARIABLE['DIR_PRETRAINED_MODELS']\n",
    "DIR_DATASETS = os.path.join(ENV_VARIABLE['DIR_DATASETS'], DOWNSTREAM_TASK)\n",
    "DIR_CHECKPOINTS = os.path.join(ENV_VARIABLE['DIR_CHECKPOINTS'], DOWNSTREAM_TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Model & Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'af-ai-center/bert-base-swedish-uncased'\n",
    "# pretrained_model_name = 'bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = get_available_datasets(DOWNSTREAM_TASK)\n",
    "available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'SUC'\n",
    "dataset = 'swedish_ner_corpus'\n",
    "\n",
    "assert dataset in available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = get_dataset_path(DIR_DATASETS, dataset)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_seq_length = 64\n",
    "num_epochs = 3\n",
    "prune_ratio = 0.02\n",
    "learning_rate = {\n",
    "    'lr_max': 2e-5,\n",
    "    'lr_schedule': 'linear_with_warmup',\n",
    "    'lr_warmup_fraction': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name, do_lower_case=False)  # needs to be False !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Processor (Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, label_list = preprocess_data(dataset_path, \n",
    "                                         tokenizer, \n",
    "                                         batch_size, \n",
    "                                         max_seq_length=max_seq_length, \n",
    "                                         prune_ratio=prune_ratio\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(pretrained_model_name, \n",
    "                                                   num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NERTrainer(model, \n",
    "                     train_dataloader=dataloader['train'], \n",
    "                     valid_dataloader=dataloader['valid'], \n",
    "                     label_list=label_list, \n",
    "                     fp16=True if torch.cuda.is_available() else False\n",
    "                    )\n",
    "\n",
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(num_epochs=num_epochs,\n",
    "            **learning_rate,\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_checkpoint(model, DIR_CHECKPOINTS, dataset, pretrained_model_name, num_epochs, prune_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(trainer, DIR_CHECKPOINTS, dataset, pretrained_model_name, num_epochs, prune_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp-swebert-applications",
   "language": "python",
   "name": "venv-nlp-swebert-applications"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
